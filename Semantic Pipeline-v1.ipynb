{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module for extracting keywords from Resofact survey data, and grouping them\n",
    "# Changes from previous versions:\n",
    "# - path for data is no longer hardcoded, it is computed based on the location of the current script\n",
    "# - reading inputs from Excel file: the number of rows is no longer hard-coded\n",
    "# - Extract keywords from each response rather than from the collection of all responses\n",
    "# - Focus on single-word keywords, not phrases\n",
    "# - Identify adjectives or nouns that modify those keywords, e.g. new products\n",
    "# - Semantically cluster these attributes\n",
    "# - If two keywords are nouns and appear one immediately after the other, we consider them a compound keyword\n",
    "# - Extract adjectives that modify these compound keywords\n",
    "# - After grouping the two keywords in the keyword compound, treat the keywords when they appear alone as any other keyword\n",
    "# - Allow acronyms in the lemmatized response text used for creating compound keywords\n",
    "\n",
    "# Load libraries:\n",
    "\n",
    "from topia.termextract import extract  # installed with pypm install topia.termextract\n",
    "# import operator\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import xlrd\n",
    "# import xlwt\n",
    "import csv\n",
    "import time\n",
    "\n",
    "\n",
    "# Function that maps Penn pos tags to WordNet pos tags:\n",
    "def map_pos(pos_var):\n",
    "    if pos_var.startswith(\"N\"):\n",
    "        return 'n'\n",
    "    elif pos_var.startswith(\"J\"):\n",
    "        return 'a'\n",
    "    elif pos_var.startswith(\"V\"):\n",
    "        return 'v'\n",
    "    elif pos_var.startswith(\"R\"):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "\n",
    "# Function that replaces a string with the sequence of lemmas of its tokens:\n",
    "\n",
    "def lemmatize_text(string, wnl_var):\n",
    "    tokens = nltk.word_tokenize(string)\n",
    "    pos_tokens = nltk.pos_tag(tokens)\n",
    "    # print \"pos_tokens:\" + str(pos_tokens)\n",
    "    my_raw_lemmas = []\n",
    "    for i in range(0, len(tokens)):\n",
    "        token = tokens[i]\n",
    "        pos = pos_tokens[i][1]\n",
    "        # print \"pos\", pos, \"token\", token\n",
    "        # my_lemma = wnl_var.lemmatize(token, pos='n')  # assume noun since that's what term algorithm returns\n",
    "        if map_pos(pos) != 'other':\n",
    "            my_lemma = wnl_var.lemmatize(token, map_pos(pos))\n",
    "        else:\n",
    "            my_lemma = wnl_var.lemmatize(token)\n",
    "\n",
    "        my_raw_lemmas.append(my_lemma)\n",
    "    # lemma_string = ' '.join(my_raw_lemmas)\n",
    "    return my_raw_lemmas\n",
    "\n",
    "\n",
    "def clean_string(string):\n",
    "    string = string.replace(\"\\r\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\").replace(\"\\n\", \".\").replace(\"..\", \".\"). \\\n",
    "        replace(\"?.\", \"?\").replace(\n",
    "        \"!.\", \"!\").replace(\".\", \". \").replace(\"?\", \"? \").replace(\"!\", \"! \")\n",
    "\n",
    "    return string\n",
    "\n",
    " \n",
    "# ------------------------------------------------------------------------\n",
    "# Instantiate a new term extractor:\n",
    "# -----------------------------------------------------------------------\n",
    "extractor = extract.TermExtractor()\n",
    "# extractor.filter = extract.DefaultFilter(singleStrengthMinOccur=2)\n",
    "extractor.filter = extract.permissiveFilter\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Instantiate a new Word Net lemmatizer:\n",
    "# -----------------------------------------------------------------------\n",
    "# wnl = nltk.stem.WordNetLemmatizer()\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Data structure for key word adjectives\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class KwAdjective:\n",
    "    def __init__(self):\n",
    "        self.key_word = None\n",
    "        self.adjectives = []\n",
    "\n",
    "    def return_adjectives(self):\n",
    "        return set(self.adjectives)\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Function that returns the list of adjectives modifying a keyword:\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "def find_adjectives_kw(kw_var, response_lemmatized, response_tagged, type_var):\n",
    "\n",
    "    kw_adjective = \"\"\n",
    "    kw_index = 0\n",
    "    if type_var == \"single\":  # if the keyword is a single word\n",
    "\n",
    "        if kw_var in response_lemmatized:\n",
    "        # get position of kw in the tokenized response:\n",
    "            kw_index = response_lemmatized.index(kw_var)\n",
    "\n",
    "    elif type_var == \"compound\": # if the keyword is a compound of two keywords\n",
    "        kw1 = kw_var.split(\" \")[0]\n",
    "        kw2 = kw_var.split(\" \")[1]\n",
    "\n",
    "        if kw1 in response_lemmatized and kw2 in response_lemmatized:\n",
    "            # I consider the index of the first item of the compound for the purpose of finding its adjectives:\n",
    "            kw_index = response_lemmatized.index(kw1)\n",
    "\n",
    "    if kw_index > 0:\n",
    "        kw_prev_index = kw_index - 1\n",
    "\n",
    "        # find adjectives modifying keywords:\n",
    "        if response_tagged[kw_prev_index][1] == 'JJ':\n",
    "            kw_adjective = response_tagged[kw_prev_index][0]\n",
    "\n",
    "    return kw_adjective\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# Functions that return the list of adjectives of the form keyword + BE + adjective:\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "def find_adjectives_be_kw(kw_var, response_lemmatized, response_tagged, type_var):\n",
    "\n",
    "    kw_adjective = \"\"\n",
    "    if kw_var in response_lemmatized:\n",
    "        # get position of kw in the tokenized response:\n",
    "        if type_var == \"single\":\n",
    "            kw_index = response_lemmatized.index(kw_var)\n",
    "            kw_next_index = kw_index + 1\n",
    "            kw_next2_index = kw_index + 2\n",
    "        elif type_var == \"compound\":\n",
    "            kw1 = kw_var.split(\" \")[0]\n",
    "            kw2 = kw_var.split(\" \")[1]\n",
    "            # I consider the index of the first item of the compound for the purpose of finding its adjectives:\n",
    "            kw_index = response_lemmatized.index(kw1)\n",
    "            kw_next_index = kw_index + 2\n",
    "            kw_next2_index = kw_index + 3\n",
    "\n",
    "        if kw_next2_index < len(response_tagged):\n",
    "            if response_tagged[kw_next2_index][1] == 'JJ' and response_lemmatized[kw_next_index] == 'be':\n",
    "                kw_adjective = response_tagged[kw_next2_index][0]\n",
    "\n",
    "    return kw_adjective\n",
    "\n",
    "\n",
    "def extract_keywords(test, target_word, input_directory, output_directory, output_file_keywords_name, output_file_freq_name, column_number, acronyms, exclude, exclude_keywords):    \n",
    "    \n",
    "    # Define directories and files:\n",
    "    \n",
    "    print (\"\\tInitializing...\")\n",
    "    # get path of current file, then change sub directory from \"code\" to \"data\"\n",
    "    \n",
    "    input_file = target_word + '.xlsx'\n",
    "        \n",
    "    # Initialize objects:\n",
    "\n",
    "    input_for_keyword_extractor = \"\"\n",
    "    id_response = {}\n",
    "    id_response_with_acronyms = {}\n",
    "    kw_list = list()\n",
    "    kw2freq = dict()\n",
    "    kw2adj2freq = dict()\n",
    "    kw_adjectives = dict()\n",
    "    kw_adj_response = dict()\n",
    "\n",
    "    # Prepare output file:\n",
    "\n",
    "    outfile = open(os.path.join(output_directory, output_file_keywords_name), 'w')\n",
    "    output = csv.writer(outfile, delimiter=',', quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "    output.writerow([\"Id\", \"Response\", \"Keyword\", \"Adjectives\"])\n",
    "\n",
    "    # Read in input file:\n",
    "    \n",
    "    sheet_name = target_word\n",
    "    row_id = 0\n",
    "    assert os.path.exists(os.path.join(input_directory, input_file)), \"I did not find the file \"+str(input_file) + \" in \" + str(input_directory)\n",
    "    sheet = xlrd.open_workbook(os.path.join(input_directory, input_file)).sheet_by_name(sheet_name)\n",
    "\n",
    "    print (\"\\tReading input...\")\n",
    "    if test == \"yes\":\n",
    "        n = 10\n",
    "    else:\n",
    "        n = sheet.nrows\n",
    "    for row_idx in range(2, n):\n",
    "    #for row_idx in range(7, 8):\n",
    "        if column_number == 0:\n",
    "            row_id += 1\n",
    "        else:\n",
    "            row_id = sheet.cell(row_idx, 0).value\n",
    "\n",
    "        print( \"\\t\\tReading response number \" + str(row_id))\n",
    "        response = sheet.cell(row_idx, column_number).value\n",
    "        \n",
    "        # response = line[2].lower()\n",
    "        # if a response has a newline, replace it with a full stop so that the keyword extractor makes sense:\n",
    "        if '\\n' in response:\n",
    "            response = clean_string(response)\n",
    "        # add a full stop at the end of each response, otherwise the keyword extractor concatenates the responses:\n",
    "        if not response.endswith(\".\") and not response.endswith(\"?\") and not response.endswith(\"!\"):\n",
    "            response += \" %s\" % \".\"\n",
    "        # if a response has a list, replace the list bullets with a full stop:\n",
    "        if re.search(r'\\d *?\\)', response):\n",
    "            response = re.sub(r'\\d *?\\)', '. ', response)\n",
    "        response = response.replace(\" - \", \". \")\n",
    "        response_lower = response.lower()\n",
    "\n",
    "        # replace -ing forms at the start of response with corresp. lemma. E.g. meeting -> meet\n",
    "        response_tokens = nltk.word_tokenize(response_lower)\n",
    "\n",
    "        if response_tokens[0].endswith('ing'):\n",
    "            start_lemma = wnl.lemmatize(response_tokens[0], pos='v')\n",
    "            response_tokens[0] = start_lemma\n",
    "            response_lower = ' '.join(response_tokens)\n",
    "        # remove acronyms from the responses:\n",
    "        response_no_acronyms = response_lower\n",
    "        for acronym in acronyms:\n",
    "            my_regex = r\"^(.+?)\\b\" + acronym + r\"\\b(.+?)$\"\n",
    "            if re.search(my_regex, response_lower):\n",
    "                # tmp_response = response_noacronyms.replace(acronym.lower(), \"\")\n",
    "                tmp_response = re.sub(my_regex, r'\\1 \\2', response_no_acronyms)\n",
    "                response_no_acronyms = tmp_response\n",
    "\n",
    "        id_response_with_acronyms[row_id] = response_lower\n",
    "\n",
    "        response_no_acronyms = response_no_acronyms.replace(\"  \", \" \").replace(\" . \", \". \").replace(\"..\", \".\").replace(\n",
    "            \" . \", \".\")\n",
    "        response_no_acronyms = response_no_acronyms.replace(\".\", \". \").replace(\"  \", \" \").replace(\" . \", \". \"). \\\n",
    "            replace(\"..\", \".\").replace(' . ', \".\").replace(\"/\", \" \")\n",
    "        response_no_acronyms = re.sub(r'[,\\.;:\\?!] *?\\. *?', '. ', response_no_acronyms)\n",
    "        response_no_acronyms = response_no_acronyms.replace(\"..\", \".\")\n",
    "        \n",
    "        response_yes_acronyms = response\n",
    "        response_yes_acronyms = response_yes_acronyms.replace(\"  \", \" \").replace(\" . \", \". \").replace(\"..\", \".\").replace(\n",
    "            \" . \", \".\")\n",
    "        response_yes_acronyms = response_yes_acronyms.replace(\".\", \". \").replace(\"  \", \" \").replace(\" . \", \". \"). \\\n",
    "            replace(\"..\", \".\").replace(' . ', \".\").replace(\"/\", \" \")\n",
    "        response_yes_acronyms = re.sub(r'[,\\.;:\\?!] *?\\. *?', '. ', response_yes_acronyms)\n",
    "        response_yes_acronyms = response_yes_acronyms.replace(\"..\", \".\")\n",
    "\n",
    "        for w in exclude:\n",
    "            my_regex = r\"^(.*?)\\b\" + w + r\"\\b(.*?)$\"\n",
    "            if re.search(my_regex, response_no_acronyms):\n",
    "                tmp_response = re.sub(my_regex, r'\\1 \\2', response_no_acronyms)\n",
    "                response_no_acronyms = tmp_response\n",
    "            if re.search(my_regex, response_yes_acronyms):\n",
    "                tmp_response = re.sub(my_regex, r'\\1 \\2', response_no_acronyms)\n",
    "                response_yes_acronyms = tmp_response\n",
    "                \n",
    "        id_response[row_id] = response_no_acronyms\n",
    "\n",
    "        # ------------------------------------------------------------------------\n",
    "        # Extract keywords:\n",
    "        # -----------------------------------------------------------------------\n",
    "        formatted_no_acronyms = re.sub(r'\\bi\\b', 'I', response_no_acronyms.lower())\n",
    "        #print \"formatted_no_acronyms:\" + str(formatted_no_acronyms)\n",
    "        my_extractor = extractor(formatted_no_acronyms)\n",
    "\n",
    "        my_response_tokenized_no_acronyms = nltk.word_tokenize(formatted_no_acronyms)\n",
    "        #print \"my_response_tokenized_no_acronyms:\" + str(my_response_tokenized_no_acronyms)\n",
    "        my_response_tagged_no_acronyms = nltk.pos_tag(my_response_tokenized_no_acronyms)\n",
    "        #print \"my_response_tagged_no_acronyms:\" + str(my_response_tagged_no_acronyms)\n",
    "        my_response_lemmatized_no_acronyms = lemmatize_text(formatted_no_acronyms, wnl)\n",
    "        #print \"my_response_lemmatized_no_acronyms:\" + str(my_response_lemmatized_no_acronyms)\n",
    "        \n",
    "        formatted_yes_acronyms = re.sub(r'\\bi\\b', 'I', response_yes_acronyms)\n",
    "        #print \"formatted_yes_acronyms:\" + str(formatted_yes_acronyms)\n",
    "        #my_extractor = extractor(formatted_yes_acronyms)\n",
    "\n",
    "        my_response_tokenized_yes_acronyms = nltk.word_tokenize(formatted_yes_acronyms)\n",
    "        my_response_tagged_yes_acronyms = nltk.pos_tag(my_response_tokenized_yes_acronyms)\n",
    "        my_response_lemmatized_yes_acronyms = lemmatize_text(formatted_yes_acronyms, wnl)\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # Select keywords:\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        kws_response = list()\n",
    "        for item in my_extractor:\n",
    "            kw = item[0]\n",
    "            print (\"\\t\\t\\tkw:\" + kw)\n",
    "            # exclude keywords that do not contain any alphabetical characters;\n",
    "            # only keep one-word keywords and exclude certain words from the list of keywords:\n",
    "            if re.search('[a-zA-Z]', kw) and len(kw.split(\" \")) < 2 and kw not in exclude_keywords:\n",
    "                kw_list.append(kw)\n",
    "                kws_response.append(kw)\n",
    "                kw2freq[kw] = kw2freq.get(kw, 0) + 1\n",
    "\n",
    "                # --------------------------------------------------\n",
    "                # find adjectives associated with this keyword:\n",
    "                # --------------------------------------------------\n",
    "                \n",
    "                #print \"response:\" + str(my_response_lemmatized_yes_acronyms)\n",
    "                kw_adj_response[kw] = \"\"\n",
    "                my_adjective = \"\"\n",
    "                my_adjective = find_adjectives_kw(kw, my_response_lemmatized_yes_acronyms, my_response_tagged_yes_acronyms, \"single\")\n",
    "                #print \"adj:\" + my_adjective\n",
    "                my_adjective_be = \"\"\n",
    "                my_adjective_be = find_adjectives_be_kw(kw, my_response_lemmatized_yes_acronyms, my_response_tagged_yes_acronyms, \"single\")\n",
    "\n",
    "                for my_adj in [my_adjective, my_adjective_be]:\n",
    "                    if my_adj != \"\" and my_adj is not None:\n",
    "                        # exclude adjectives that do not contain any alphabetical characters:\n",
    "                        if re.search('[a-zA-Z]', my_adj):\n",
    "                            kw_adj_response[kw] = my_adj.lower().rstrip()\n",
    "                            if (kw, my_adj) in kw2adj2freq:\n",
    "                                kw2adj2freq[(kw, my_adj)] += 1\n",
    "                            else:\n",
    "                                kw2adj2freq[(kw, my_adj)] = 1\n",
    "                            if kw in kw_adjectives:\n",
    "                                kw_adjectives[kw].adjectives.append(my_adj)\n",
    "                            else:\n",
    "                                kw_a_object = KwAdjective()\n",
    "                                kw_a_object.key_word = kw\n",
    "                                kw_a_object.adjectives.append(my_adj)\n",
    "                                kw_adjectives[kw] = kw_a_object\n",
    "                                \n",
    "        # -----------------------------------------------------\n",
    "        # Add acronyms to the list of keywords:\n",
    "        # -----------------------------------------------------\n",
    "\n",
    "        for acronym in acronyms:\n",
    "            freq_acronym = 0\n",
    "            response_withacronyms = response\n",
    "            my_regex = r\".+?\\b\" + acronym + r\"\\b.+?\"\n",
    "            if re.search(my_regex, response_lower):\n",
    "                kw_list.append(acronym)\n",
    "                kws_response.append(acronym)\n",
    "                kw2freq[acronym] = kw2freq.get(acronym, 0) + 1\n",
    "\n",
    "        # If two keywords are nouns and appear one immediately after the other, we consider them a compound keyword;\n",
    "        # e.g. \"brand name\", \"track record\":\n",
    "        \n",
    "        #print my_response_lemmatized_yes_acronyms\n",
    "        for kw1 in kws_response:\n",
    "            for kw2 in kws_response:\n",
    "                if kw1 != kw2 and len(kw1.split(\" \")) == 1 and len(kw2.split(\" \")) == 1:\n",
    "                    compound_kw = \"\"\n",
    "                    try:\n",
    "                        index_kw1 = my_response_lemmatized_yes_acronyms.index(kw1)\n",
    "                        index_kw2 = my_response_lemmatized_yes_acronyms.index(kw2)\n",
    "                        #print \"index 1:\" + str(index_kw1)\n",
    "                        #print \"index 2:\" + str(index_kw2)                        \n",
    "                        if index_kw2 > 1 and index_kw1 == index_kw2 - 1:\n",
    "                            compound_kw = kw1 + \" \" + kw2\n",
    "                        elif index_kw1 > 1 and index_kw2 == index_kw1 - 1:\n",
    "                            compound_kw = kw2 + \" \" + kw1\n",
    "                        #print \"compound:\" + compound_kw\n",
    "                    except:\n",
    "                        #print \"Error for \" + kw1 + \" and \" + kw2\n",
    "                        compound_kw = \"\"\n",
    "                        \n",
    "                    if compound_kw != \"\":\n",
    "                        kws_response.append(compound_kw)\n",
    "                        kw_list.append(compound_kw)\n",
    "                        kw2freq[compound_kw] = kw2freq.get(compound_kw, 0) + 1\n",
    "                        #print \"frequency:\" + str(kw2freq[compound_kw])\n",
    "\n",
    "                        # add adjectives of compound keyword:\n",
    "                        kw_adj_response[compound_kw] = \"\"\n",
    "                        try:\n",
    "                            my_adjective_c = find_adjectives_kw(compound_kw, my_response_lemmatized_yes_acronyms, my_response_tagged_yes_acronyms, \"compound\")\n",
    "                        except:\n",
    "                            my_adjective_c = \"\"\n",
    "                        #print \"adjective:\" + my_adjective_c\n",
    "                        try:\n",
    "                            my_adjective_c_be = find_adjectives_be_kw(compound_kw, my_response_lemmatized_yes_acronyms, my_response_tagged_yes_acronyms, \"compound\")\n",
    "                        except:\n",
    "                            my_adjective_c_be = \"\"\n",
    "                        #print \"adjective be:\" + my_adjective_c_be\n",
    "\n",
    "                        for my_adj_c in [my_adjective_c, my_adjective_c_be]:\n",
    "                            if my_adj_c != \"\":\n",
    "                                if re.search('[a-zA-Z]', my_adj_c):\n",
    "                                    kw_adj_response[compound_kw] = my_adj_c.lower()\n",
    "                                    if (compound_kw, my_adj_c) in kw2adj2freq:\n",
    "                                        kw2adj2freq[(compound_kw, my_adj_c)] += 1\n",
    "                                    else:\n",
    "                                        kw2adj2freq[(compound_kw, my_adj_c)] = 1\n",
    "                                    if compound_kw in kw_adjectives:\n",
    "                                        kw_adjectives[compound_kw].adjectives.append(my_adj_c)\n",
    "                                    else:\n",
    "                                        kw_a_object = KwAdjective()\n",
    "                                        kw_a_object.key_word = compound_kw\n",
    "                                        kw_a_object.adjectives.append(my_adj_c)\n",
    "                                        kw_adjectives[compound_kw] = kw_a_object\n",
    "                    \n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # Print out keywords:\n",
    "        # --------------------------------------------------\n",
    "        kws_response = set(kws_response)\n",
    "        for kw in kws_response:\n",
    "            output.writerow([row_id, response, kw, kw_adj_response.get(kw, \"\")])\n",
    "\n",
    "    outfile.close()\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Open output file with keywords' frequency:\n",
    "    # ---------------------------------------------\n",
    "    \n",
    "    print (\"\\tWriting output...\")\n",
    "    outfile_freq = open(os.path.join(output_directory, output_file_freq_name), 'w')\n",
    "    output_freq = csv.writer(outfile_freq, delimiter=',', quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "    output_freq.writerow(\n",
    "        [\"Keyword\", \"Number of responses of keyword\", \"Adjective\", \"Number of responses of keyword and adjective\"])\n",
    "    kw_list = sorted(list(set(kw_list)))\n",
    "\n",
    "    count = 0\n",
    "    for kw in kw_list:\n",
    "        count += 1\n",
    "        print (\"\\t\\t\" + str(count) + \" out of \" + str(len(kw_list)) + \": Printing frequencies: \" + kw)\n",
    "        if kw in kw_adjectives:\n",
    "            my_adj_list = kw_adjectives[kw].return_adjectives()\n",
    "        else:\n",
    "            my_adj_list = list()\n",
    "\n",
    "        if len(my_adj_list) > 0:\n",
    "            for adj in my_adj_list:\n",
    "                output_freq.writerow([kw, str(kw2freq[kw]), adj, str(kw2adj2freq[(kw, adj)])])\n",
    "        else:\n",
    "            output_freq.writerow([kw, str(kw2freq[kw]), \"\", \"\"])\n",
    "\n",
    "    outfile_freq.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for data processing and analysis for Resofact\n",
    "# Version: 1.2\n",
    "# Date: 27/11/2016\n",
    "# Authors: Gard Jenset and Barbara McGillivray\n",
    "\n",
    "# ----------------------------------\n",
    "# Import modules:\n",
    "# ----------------------------------\n",
    "\n",
    "# For keyword extraction:\n",
    "\n",
    "from __future__ import division\n",
    "from topia.termextract import extract  # installed with pypm install topia.termextract\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import xlrd\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# For semantic clustering:\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "#import os\n",
    "#import csv\n",
    "#import time\n",
    "import semantic_similarity_functions # our module\n",
    "\n",
    "# For word clouds:\n",
    "\n",
    "#import os\n",
    "#import csv\n",
    "from pytagcloud import create_tag_image, make_tags\n",
    "from pytagcloud.lang.counter import get_tag_counts\n",
    "\n",
    "# For bar plots:\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "#import numpy as np\n",
    "#import os\n",
    "#import csv\n",
    "\n",
    "# ---------------------------------------\n",
    "# Command-line interface:\n",
    "# ---------------------------------------\n",
    "\n",
    "# import argparse\n",
    "\n",
    "# # instantiate a new argument parser\n",
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# # these are the options for the interface:\n",
    "# parser.add_argument('--dir', '-d', required=True, help='Full path to directory containing DT HTML output files.')\n",
    "# parser.add_argument('--weekly', '-w', required=False, action=\"store_true\",\n",
    "                    # help='''Time period to do report for: Including it will accumulate results\n",
    "                    # within the same week. Excluding it will accumulate results within the same day.''')\n",
    "# parser.add_argument('--clean', '-c', required=False,\n",
    "                    # help='Optional string/regex to clean from start of DT html filename for readability. E.g.: HP_DT_.')\n",
    "\n",
    "# # parse what's been entered...\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# # can now do...\n",
    "# args.dir\n",
    "# args.weekly\n",
    "# args.clean\n",
    "\n",
    "\n",
    "# Parameters:\n",
    "\n",
    "test = input(\"Is this a test? Reply yes or no. Leave empty for yes.\")\n",
    "target_word = input(\"What is the target word for the survey? Leave empty for pride\")  # 'embarassment'#'success'\n",
    "    \n",
    "if target_word == \"\":\n",
    "    target_word = \"pride\"\n",
    "    \n",
    "input_directory = input(\"What is the path to the input directory where the spread sheet with the responses is? Leave empty for default (company_name\\company_name_pride\\). The input spread sheet should have the same name as the target word and the responses should be in the sheet whose name is the target word.\")\n",
    "output_directory = input(\"What is the path to the output directory where the output files should be saved? Leave empty for default (company_name\\company_name_pride\\).\")\n",
    "acronyms_file = input(\"What is the name of the file containing the acronyms to be excluded from the list of keywords? Leave empty for default (hard_coded here or create acronyms_list_default.txt). Note that this file should be in the input folder.\")\n",
    "min_similarity_score = input(\"What is the minimum threshold for the semantic similarity score? Leave empty for 0.8.\")\n",
    "column_number = input(\"What is the number of the column containing the survey responses in the input spread sheet? Leave empty for 0.\")  # 12\n",
    "n_words = input(\"What is the maximum number of words to include in the plots? Leave empty for 10.\")\n",
    "min_freq = input(\"What is the frequency threshold above which you want to view keywords in the network graph? Leave empty for 3.\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# File and directory names\n",
    "# ---------------------------------------\n",
    "   \n",
    "#path_keyword_freq = path + r'\\output\\company_name\\company_name_' + target_word\n",
    "output_file_freq_name = 'Keywords_frequency_' + target_word + \"_\" + time.strftime(\"%d%m%Y\") + \".csv\"\n",
    "output_word_cloud = 'word_cloud_' + target_word + \"_\" + time.strftime(\"%d%m%Y\") + \".png\"\n",
    "output_bar_plot = 'bar_plot_' + target_word + \"_\" + time.strftime(\"%d%m%Y\") + \".png\"\n",
    "output_graph = 'graph_' + target_word + \"_\" + time.strftime(\"%d%m%Y\") + \"_freqthreshold\" + str(min_freq) + \".png\"\n",
    "output_file_cluster = 'Keywords_frequency_'+ target_word + \"_\" + time.strftime(\"%d%m%Y\") + \"_clustered.csv\"\n",
    "output_file_keywords_name = 'Responses_keywords_' + target_word + \"_\" + time.strftime(\"%d%m%Y\") + \".csv\"\n",
    "output_file_sim_name = 'Keywords_frequency_'+ target_word + \"_\" + time.strftime(\"%d%m%Y\") + \"_similarities.csv\"\n",
    "\n",
    "# Default parameters:\n",
    "\n",
    "if test == \"\":\n",
    "    test = \"yes\"\n",
    "    \n",
    "if min_similarity_score == \"\":\n",
    "    min_similarity_score = 0.8\n",
    "    \n",
    "if column_number == \"\":\n",
    "    column_number = 0\n",
    "else:\n",
    "    column_number = int(column_number)\n",
    "    \n",
    "if n_words == \"\":\n",
    "    n_words = 10\n",
    "else:\n",
    "    n_words = int(n_words)\n",
    "    \n",
    "if acronyms_file == \"\":\n",
    "    acronyms_file = \"acronyms_list_default.txt\"\n",
    "    \n",
    "if input_directory == \"\":\n",
    "    path = os.getcwd().replace(\"code\", \"data\")\n",
    "    input_directory = 'company_name\\\\company_name_pride\\\\'\n",
    "    \n",
    "if output_directory == \"\":\n",
    "    path = os.getcwd().replace(\"code\", \"data\")\n",
    "    output_directory = 'company_name\\\\company_name_pride\\\\'\n",
    "    \n",
    "assert os.path.exists(input_directory), \"I did not find the input directory \"+str(input_directory)\n",
    "assert os.path.exists(output_directory), \"I did not find the output directory \"+str(output_directory)\n",
    "\n",
    "#path = os.getcwd().replace(\"code\", \"data\")\n",
    "#path_plots = os.getcwd() + r'\\plots'\n",
    "path_plots = os.path.join(output_directory, \"plots\")\n",
    "if not os.path.exists(path_plots):\n",
    "    os.makedirs(path_plots)\n",
    "    \n",
    "if min_freq == \"\":\n",
    "    min_freq = 3\n",
    "else:\n",
    "    min_freq = int(min_freq)\n",
    "    \n",
    "# Test files:    \n",
    "    \n",
    "if test == \"yes\":\n",
    "    output_file_freq_name = output_file_freq_name.replace(\".csv\", \"_test.csv\")\n",
    "    output_word_cloud = output_word_cloud.replace(\".csv\", \"_test.csv\")\n",
    "    output_bar_plot = output_bar_plot.replace(\".csv\", \"_test.csv\")\n",
    "    output_file_cluster = output_file_cluster.replace(\".csv\", \"_test.csv\")\n",
    "    output_file_keywords_name = output_file_keywords_name.replace(\".csv\", \"_test.csv\")\n",
    "    output_file_sim_name = output_file_sim_name.replace(\".csv\", \"_test.csv\")\n",
    "    \n",
    "# ----------------------------------------------------------\n",
    "# Exclusion lists\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# List of words to be excluded, for example \"e.g.\"\n",
    "\n",
    "exclude = ['e. g. ', 'e.g.']\n",
    "\n",
    "# Words to be excluded as keywords:\n",
    "\n",
    "exclude_keywords = ['i']\n",
    "\n",
    "# Exlude names of companies:\n",
    "\n",
    "if target_word == \"pride\" or target_word == \"embarassment\":\n",
    "    exclude_keywords.append('replace_with_company_name')\n",
    "   \n",
    "# List of acronyms that should not be lemmatized:\n",
    "#for many acronyms create a file in input_directory\n",
    "#assert os.path.exists(os.path.join(input_directory, acronyms_file)), \"I did not find the file for acronyms \"+str(acronyms_file) + \" in \" + str(input_directory)\n",
    "#with open(os.path.join(input_directory, acronyms_file)) as acronyms_f:\n",
    "#    acronyms = acronyms_f.read().splitlines()\n",
    "acronyms = ['SA', 'HR', 'ICT', 'IT']\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Extract keywords\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "#from resofact_topic_extraction_27112016 import extract_keywords\n",
    "\n",
    "# Extract keywords and write them to a file:\n",
    "\n",
    "print (\"Extracting keywords...\")\n",
    "skip = input(\"Do you want to skip this step? Leave empty if you want this.\")\n",
    "if skip != \"yes\" and skip != \"\":\n",
    "    extract_keywords(test, target_word, input_directory, output_directory, output_file_keywords_name, output_file_freq_name, \n",
    "    column_number, acronyms, exclude, exclude_keywords)\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# Calculate semantic similarity:\n",
    "# -----------------------------------\n",
    "\n",
    "from semantic_similarity_27112016 import calculate_keyword_semantic_similarity\n",
    "\n",
    "print( \"Calculating semantic similarity between keywords...\")\n",
    "skip = input(\"Do you want to skip this step? Leave empty if you want this.\")\n",
    "if skip != \"yes\" and skip != \"\":\n",
    "    calculate_keyword_semantic_similarity(test, target_word, output_directory, output_file_freq_name, output_file_sim_name, min_similarity_score)\n",
    "\n",
    "# ----------------------------------\n",
    "# Cluster keywords:\n",
    "# ----------------------------------\n",
    "\n",
    "from clustering_27112016 import cluster_keywords\n",
    "\n",
    "print( \"Clustering keywords...\")\n",
    "skip = input(\"Do you want to skip this step? Leave empty if you want this.\")\n",
    "if skip != \"yes\" and skip != \"\":\n",
    "    cluster_keywords(test, target_word, min_similarity_score, output_directory, output_file_freq_name, output_file_sim_name, output_file_cluster)\n",
    "    \n",
    "# ---------------------------------\n",
    "# Create word clouds:\n",
    "# ---------------------------------\n",
    "\n",
    "print( \"Creating word clouds...\")\n",
    "\n",
    "from create_word_cloud_23102016 import create_word_cloud\n",
    "\n",
    "skip = input(\"Do you want to skip this step? Leave empty if you want this.\")\n",
    "if skip != \"yes\" and skip != \"\":\n",
    "    # One for clusters (clusters include singletons) and their frequencies:\n",
    "    #create_word_cloud(path_keyword_freq, output_file_freq_name, path_plots, output_word_cloud)\n",
    "    create_word_cloud(output_directory, output_file_freq_name, path_plots, output_word_cloud.replace(\".png\", \"_keywords.png\"), \"keywords\")\n",
    "    create_word_cloud(output_directory, output_file_cluster, path_plots, output_word_cloud.replace(\".png\", \"_clusters.png\"), \"clusters\")\n",
    "    \n",
    "\n",
    "# --------------------------------\n",
    "# Create bar plots:\n",
    "# --------------------------------\n",
    "\n",
    "print (\"Creating bar plots...\")\n",
    "\n",
    "from create_bar_plot_23102016 import create_bar_plot\n",
    "\n",
    "skip = input(\"Do you want to skip this step? Leave empty if you want this.\")\n",
    "if skip != \"yes\" and skip != \"\":\n",
    "    create_bar_plot(output_directory, output_file_freq_name, path_plots, output_bar_plot.replace(\".png\", \"_keywords.png\"), n_words, \"keywords\")\n",
    "    create_bar_plot(output_directory, output_file_cluster, path_plots, output_bar_plot.replace(\".png\", \"_clusters.png\"), n_words, \"clusters\")\n",
    "\n",
    "    \n",
    "# --------------------------------\n",
    "# Create graphs:\n",
    "# --------------------------------\n",
    "\n",
    "print (\"Creating graphs...\")\n",
    "\n",
    "from create_graph_27112016 import create_graph\n",
    "\n",
    "skip = input(\"Do you want to skip this step? Leave empty if you want this.\")\n",
    "if skip != \"yes\" and skip != \"\":\n",
    "    create_graph(output_directory, output_file_keywords_name, path_plots, output_graph.replace(\".png\", \"_keywords.png\"), min_freq)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
